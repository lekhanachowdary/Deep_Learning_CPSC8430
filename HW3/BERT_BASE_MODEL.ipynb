{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f9db4d-4642-456a-a228-bdaecdf89bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsingam/.local/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/lsingam/.local/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/lsingam/.local/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/lsingam/.local/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/lsingam/.local/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 4639/4639 [02:48<00:00, 27.58it/s]\n",
      "Evaluating: 100%|██████████| 15875/15875 [00:52<00:00, 303.22it/s]\n",
      "INFO:__main__:Epoch 1/5\n",
      " Train Loss: 5.9288, F1 Score: 0.2749\n",
      "Training: 100%|██████████| 4639/4639 [02:47<00:00, 27.63it/s]\n",
      "Evaluating: 100%|██████████| 15875/15875 [00:51<00:00, 308.82it/s]\n",
      "INFO:__main__:Epoch 2/5\n",
      " Train Loss: 5.3574, F1 Score: 0.3890\n",
      "Training: 100%|██████████| 4639/4639 [02:47<00:00, 27.63it/s]\n",
      "Evaluating: 100%|██████████| 15875/15875 [00:51<00:00, 310.94it/s]\n",
      "INFO:__main__:Epoch 3/5\n",
      " Train Loss: 4.8187, F1 Score: 0.4487\n",
      "Training: 100%|██████████| 4639/4639 [02:47<00:00, 27.64it/s]\n",
      "Evaluating: 100%|██████████| 15875/15875 [00:51<00:00, 311.00it/s]\n",
      "INFO:__main__:Epoch 4/5\n",
      " Train Loss: 4.3948, F1 Score: 0.4815\n",
      "Training: 100%|██████████| 4639/4639 [02:47<00:00, 27.65it/s]\n",
      "Evaluating: 100%|██████████| 15875/15875 [00:50<00:00, 312.13it/s]\n",
      "INFO:__main__:Epoch 5/5\n",
      " Train Loss: 4.0687, F1 Score: 0.4831\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def load_dataset_from_file(filepath):\n",
    "    \"\"\"Loads the dataset from a given JSON file containing QA data.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {filepath}\")\n",
    "        raise\n",
    "\n",
    "    context_list, question_list, answer_list = [], [], []\n",
    "    total_questions = 0\n",
    "    total_possible = 0\n",
    "    total_impossible = 0\n",
    "\n",
    "    for section in dataset['data']:\n",
    "        for paragraph in section['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qna in paragraph['qas']:\n",
    "                question = qna['question']\n",
    "                total_questions += 1\n",
    "                if 'is_impossible' in qna and qna['is_impossible']:\n",
    "                    total_impossible += 1\n",
    "                else:\n",
    "                    total_possible += 1\n",
    "                for answer in qna.get('answers', []):\n",
    "                    context_list.append(context.lower())\n",
    "                    question_list.append(question.lower())\n",
    "                    answer_list.append(answer)\n",
    "\n",
    "    return total_questions, total_possible, total_impossible, context_list, question_list, answer_list\n",
    "\n",
    "\n",
    "def calculate_end_positions(answer_data, context_data):\n",
    "    \"\"\"Computes the end position of the answers based on the answer start position and answer text.\"\"\"\n",
    "    for answer, context in zip(answer_data, context_data):\n",
    "        answer_text = answer.get('text', '').lower()\n",
    "        answer_start = answer.get('answer_start', -1)\n",
    "        answer['answer_end'] = answer_start + len(answer_text)\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_total, train_possible, train_impossible, train_contexts, train_questions, train_answers = load_dataset_from_file('spoken_train-v1.1.json')\n",
    "    valid_total, valid_possible, valid_impossible, valid_contexts, valid_questions, valid_answers = load_dataset_from_file('spoken_test-v1.1.json')\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply answer end position calculation\n",
    "calculate_end_positions(train_answers, train_contexts)\n",
    "calculate_end_positions(valid_answers, valid_contexts)\n",
    "\n",
    "MAX_SEQ_LEN = 512\n",
    "MODEL_PATH = \"distilbert-base-uncased\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Tokenize training and validation sets\n",
    "train_encodings = tokenizer(train_questions, train_contexts, max_length=MAX_SEQ_LEN, padding=True, truncation=True)\n",
    "valid_encodings = tokenizer(valid_questions, valid_contexts, max_length=MAX_SEQ_LEN, padding=True, truncation=True)\n",
    "\n",
    "# Custom Dataset class\n",
    "class QAData(Dataset):\n",
    "    def __init__(self, encodings, answers):\n",
    "        \"\"\"\n",
    "        Custom dataset for question answering task.\"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.answers = answers\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches a specific item from the dataset.\"\"\"\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['start_positions'] = torch.tensor(self.answers[idx].get('answer_start', -1))\n",
    "        item['end_positions'] = torch.tensor(self.answers[idx].get('answer_end', -1))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = QAData(train_encodings, train_answers)\n",
    "valid_dataset = QAData(valid_encodings, valid_answers)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "# Load pre-trained model and move to device\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer):\n",
    "    \"\"\" Trains the model for one epoch.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(data_loader, desc='Training'):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                         start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_on_validation_set(model, data_loader):\n",
    "    \"\"\"Evaluates the model on the validation set and computes F1 score.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    f1_scores = []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        for i in range(len(start_true)):\n",
    "            predicted_answer = tokenizer.decode(input_ids[i][start_pred[i]:end_pred[i] + 1], skip_special_tokens=True)\n",
    "            true_answer = tokenizer.decode(input_ids[i][start_true[i]:end_true[i] + 1], skip_special_tokens=True)\n",
    "\n",
    "            predicted_tokens = set(predicted_answer.strip().split())\n",
    "            true_tokens = set(true_answer.strip().split())\n",
    "\n",
    "            if true_tokens and predicted_tokens:\n",
    "                intersection = predicted_tokens.intersection(true_tokens)\n",
    "                precision = len(intersection) / len(predicted_tokens) if len(predicted_tokens) > 0 else 0\n",
    "                recall = len(intersection) / len(true_tokens) if len(true_tokens) > 0 else 0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 5\n",
    "best_f1 = 0.0\n",
    "patience = 3\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_f1 = evaluate_on_validation_set(model, valid_loader)\n",
    "    logger.info(f\"Epoch {epoch + 1}/{EPOCHS}\\n Train Loss: {train_loss:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        if early_stop_count >= patience:\n",
    "            logger.info(\"Early stopping activated!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed5f54-21ea-4e2e-acdc-809a9acc2aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
